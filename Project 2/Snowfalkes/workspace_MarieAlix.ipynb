{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the MSCDB api\n",
    "import mascdb.api\n",
    "from mascdb.api import MASC_DB\n",
    "\n",
    "# Import other libraries\n",
    "from IPython.display import display\n",
    "import pyarrow\n",
    "\n",
    "# Import files\n",
    "from helpers import *\n",
    "from models import *\n",
    "from cross_validation import *\n",
    "\n",
    "# Autoreload\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drop duplicates\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the classifier\n",
    "classifier  = 'hydro'\n",
    "\n",
    "# Get the data and the correpsonding classes\n",
    "mascdb_data = pd.read_pickle('Data/data_set.pkl')\n",
    "mascdb_classes = pd.read_pickle('Data/classes.pkl').reset_index(drop = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all the wrong duplicates flakes\n",
    "mascdb_classes_copy = mascdb_classes.copy()\n",
    "\n",
    "mascdb_classes_copy_1 = mascdb_classes_copy[mascdb_classes_copy.duplicated(subset = None, keep = False)]\n",
    "mascdb_classes_copy_2 = mascdb_classes_copy[mascdb_classes_copy.duplicated(subset=['flake_id'], keep = False)]\n",
    "\n",
    "mascdb_classes_wrong_duplicates = pd.concat([mascdb_classes_copy_1, mascdb_classes_copy_2]).drop_duplicates(keep = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(mascdb_classes_wrong_duplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the flake id of the wrong duplicates\n",
    "mascdb_classes_wrong_duplicates_unique = mascdb_classes_wrong_duplicates.drop_duplicates(subset = ['flake_id'], keep = 'first')\n",
    "\n",
    "# Get all the flake id with classes\n",
    "flake_id_classes = mascdb_classes_copy.drop_duplicates(subset=['flake_id'], keep = 'first')\n",
    "\n",
    "# Remove the wrong flake id from all the flake id\n",
    "mascdb_classes_modified = pd.concat([flake_id_classes, mascdb_classes_wrong_duplicates_unique]).drop_duplicates(subset=['flake_id'], keep = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, we want to be sure to have one class for each snowflakes\n",
    "mascdb_data_modified = mascdb_data[mascdb_data.flake_id.isin(mascdb_classes_modified.flake_id)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transform the data (standardization)\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform the data\n",
    "mascdb_data_modified_copy = mascdb_data_modified.copy()\n",
    "power_transformer = preprocessing.PowerTransformer(method = 'yeo-johnson', standardize = True)\n",
    "mascdb_data_modified_std = power_transformer.fit(mascdb_data_modified_copy.drop(['flake_id'], axis=1))\n",
    "mascdb_data_modified_std = power_transformer.transform(mascdb_data_modified_copy.drop(['flake_id'], axis=1))\n",
    "\n",
    "# Set the transformed data\n",
    "mascdb_data_modified[mascdb_data_modified.columns.difference(['flake_id'])]  = mascdb_data_modified_std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelization\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into a data set X_ and a response set y_\n",
    "X_ = mascdb_data_modified[mascdb_data_modified.columns.difference(['flake_id'])]\n",
    "y_ = mascdb_classes_modified.copy().set_index('flake_id')\n",
    "\n",
    "# Get a column as flake_id\n",
    "X_['flake_id'] = X_.index\n",
    "\n",
    "# Supress all the duplicates flake_id and get the correponding class\n",
    "X_ = X_.drop_duplicates(subset = 'flake_id', keep = 'first').join(y_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into a data set X and a response set y\n",
    "y = pd.DataFrame(X_['class_id'])\n",
    "X = X_[X_.columns.difference(['flake_id', 'class_id'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split data and Transform\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using our split data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a train and test set for modelization\n",
    "X_train, y_train, X_test, y_test = split_data(X, y, n_s = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a train and test set for modelization\n",
    "X_train_bis, X_test_bis, y_train_bis, y_test_bis = train_test_split(X, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transforme the y_train and the y_test after split because StratifiedKFold can only deal with 1D array\n",
    "# can be used for SVM but work also well without the transformed y\n",
    "y_train_transformed = classes_transformed(y_train)\n",
    "y_test_transformed = classes_transformed(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SMOTE for oversampling imbalanced classification datasets\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_rs, y_train_rs = smote_data_augmentation(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature augmentation and selection \n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature importance \n",
    "https://machinelearningmastery.com/calculate-feature-importance-with-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "estimator = \n",
    "sfs = SequentialFeatureSelector(estimator).fit(X_train, y_train)\n",
    "X_train_selec = sfs.transform(X_train)\n",
    "X_test_selec = sfs.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature augmentation\n",
    "poly = PolynomialFeatures(2)\n",
    "X_poly = pd.DataFrame(poly.fit_transform(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split data after feature augmentation\n",
    "X_poly_train, y_train, X_poly_test, y_test = split_data(X_poly, y, n_s = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_poly_train_rs, y_train_rs = smote_data_augmentation(X_poly_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model selection without feature augmentation\n",
    "model_selec = get_model_features_selection(X_train_rs, y_train_rs, 'recursiveCV', 5)\n",
    "X_train_selec = model_selec.transform(X_train_rs)\n",
    "X_test_selec = model_selec.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model selection without feature augmentation\n",
    "model_selec = get_model_features_selection(X_poly_train_rs, y_train_rs, 'recursive')\n",
    "X_poly_train_selec = model_selec.transform(X_poly_train_rs)\n",
    "X_poly_test_selec = model_selec.transform(X_poly_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression \n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tune regularization for multinomial logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR, param = get_model_LR(True)\n",
    "cv = evaluate_model(LR, param, X_train_rs, y_train_rs, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model('LR.pkl', clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('LR.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(model.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm, param = get_model_SVM(True)\n",
    "evaluate_model(svm, param, X_train, y_train, X_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
